{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4abe09a9",
   "metadata": {},
   "source": [
    "## ä¸€ã€è¼‰å…¥å¥—ä»¶èˆ‡è¨­å®šè·¯å¾‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12738cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ è³‡æ–™è®€å–è·¯å¾‘è¨­å®šç‚º: ../DataSet/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# å¿½ç•¥ pandas çš„è­¦å‘Šè¨Šæ¯\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- ğŸ“‚ Step 1: è¨­å®šæª”æ¡ˆè·¯å¾‘ ---\n",
    "# æ ¹æ“šæ‚¨çš„æŒ‡ç¤ºï¼Œç›´æ¥è¨­å®šç›¸å°è·¯å¾‘\n",
    "data_dir = \"../DataSet/\"\n",
    "output_dir = \"./\" # è¼¸å‡ºåˆ°ç•¶å‰ç›®éŒ„\n",
    "\n",
    "# ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(f\"ğŸ“‚ è³‡æ–™è®€å–è·¯å¾‘è¨­å®šç‚º: {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8236c2b8",
   "metadata": {},
   "source": [
    "## äºŒã€è®€å–åŸå§‹è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14bd480b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ æ­£åœ¨è®€å–è³‡æ–™...\n",
      "âœ… è³‡æ–™è®€å–å®Œæˆï¼\n",
      "   - äº¤æ˜“ç´€éŒ„: 4435890 ç­†\n",
      "   - è­¦ç¤ºå¸³æˆ¶: 1004 ç­†\n",
      "   - å¾…é æ¸¬å¸³æˆ¶: 4780 ç­†\n"
     ]
    }
   ],
   "source": [
    "# --- ğŸ“¥ Step 2: è®€å–è³‡æ–™ ---\n",
    "print(\"â³ æ­£åœ¨è®€å–è³‡æ–™...\")\n",
    "\n",
    "try:\n",
    "    # 1. è®€å–äº¤æ˜“ç´€éŒ„\n",
    "    txn_path = os.path.join(data_dir, 'acct_transaction.csv')\n",
    "    df_txn = pd.read_csv(txn_path, dtype={\n",
    "        'from_acct': str, 'to_acct': str, \n",
    "        'from_acct_type': str, 'to_acct_type': str,\n",
    "        'txn_amt': float\n",
    "    })\n",
    "\n",
    "    # 2. è®€å–è­¦ç¤ºå¸³æˆ¶ (æ¨™ç±¤è³‡æ–™)\n",
    "    alert_path = os.path.join(data_dir, 'acct_alert.csv')\n",
    "    df_alert = pd.read_csv(alert_path, dtype={'acct': str})\n",
    "\n",
    "    # 3. è®€å–å¾…é æ¸¬å¸³æˆ¶ (æäº¤ç›®æ¨™)\n",
    "    # åŠ å…¥æ¬„ä½åç¨±æª¢æŸ¥æ©Ÿåˆ¶ï¼Œç¢ºä¿èƒ½æ­£ç¢ºè®€å– ID\n",
    "    predict_path = os.path.join(data_dir, 'acct_predict.csv')\n",
    "    # å…ˆè®€å– header ç¢ºèªæ¬„ä½\n",
    "    df_predict_raw = pd.read_csv(predict_path, nrows=0)\n",
    "    predict_cols = df_predict_raw.columns.tolist()\n",
    "    \n",
    "    # åˆ¤æ–· ID æ¬„ä½åç¨±\n",
    "    if 'acct_test' in predict_cols:\n",
    "        pid = 'acct_test'\n",
    "    elif 'acct' in predict_cols:\n",
    "        pid = 'acct'\n",
    "    else:\n",
    "        pid = predict_cols[0] # é è¨­ç¬¬ä¸€å€‹æ¬„ä½\n",
    "    \n",
    "    df_predict = pd.read_csv(predict_path, dtype={pid: str})\n",
    "    if pid != 'acct_test':\n",
    "        df_predict = df_predict.rename(columns={pid: 'acct_test'})\n",
    "\n",
    "    print(f\"âœ… è³‡æ–™è®€å–å®Œæˆï¼\")\n",
    "    print(f\"   - äº¤æ˜“ç´€éŒ„: {df_txn.shape[0]} ç­†\")\n",
    "    print(f\"   - è­¦ç¤ºå¸³æˆ¶: {df_alert.shape[0]} ç­†\")\n",
    "    print(f\"   - å¾…é æ¸¬å¸³æˆ¶: {df_predict.shape[0]} ç­†\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆï¼Œè«‹ç¢ºèªè·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚\\nè©³ç´°éŒ¯èª¤: {e}\")\n",
    "    # ç‚ºäº†è®“ä¸‹é¢çš„ cell ä¸å ±éŒ¯ï¼Œå®šç¾©ç©ºçš„ DataFrame (åƒ…åœ¨å‡ºéŒ¯æ™‚)\n",
    "    df_txn = pd.DataFrame()\n",
    "    df_alert = pd.DataFrame()\n",
    "    df_predict = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100ff2b",
   "metadata": {},
   "source": [
    "## ä¸‰ã€ç‰¹å¾µå·¥ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09750f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ é–‹å§‹è¨ˆç®— V4 å®˜æ–¹ç‰¹å¾µ...\n",
      "   è¨ˆç®—ä»˜æ¬¾æ–¹ç‰¹å¾µ...\n",
      "   è¨ˆç®—æ”¶æ¬¾æ–¹ç‰¹å¾µ...\n",
      "   åˆä½µç‰¹å¾µ...\n",
      "âœ… ç‰¹å¾µè¨ˆç®—å®Œæˆï¼å…± 1800106 å€‹å¸³æˆ¶ã€‚\n"
     ]
    }
   ],
   "source": [
    "# --- ğŸ› ï¸ Step 3: ç‰¹å¾µå·¥ç¨‹ (Feature Engineering) ---\n",
    "if not df_txn.empty:\n",
    "    print(\"âš™ï¸ é–‹å§‹è¨ˆç®— V4 å®˜æ–¹ç‰¹å¾µ...\")\n",
    "\n",
    "    # 1. è™•ç†ç‰å±±å¸³æˆ¶æ¨™è¨˜\n",
    "    df_txn['is_esun_from'] = (df_txn['from_acct_type'] == '01').astype(int)\n",
    "    df_txn['is_esun_to'] = (df_txn['to_acct_type'] == '01').astype(int)\n",
    "\n",
    "    # --- ä»˜æ¬¾æ–¹ç‰¹å¾µ ---\n",
    "    print(\"   è¨ˆç®—ä»˜æ¬¾æ–¹ç‰¹å¾µ...\")\n",
    "    debit_features = df_txn.groupby('from_acct').agg(\n",
    "        total_send_amt=('txn_amt', 'sum'),\n",
    "        max_send_amt=('txn_amt', 'max'),\n",
    "        min_send_amt=('txn_amt', 'min'),\n",
    "        avg_send_amt=('txn_amt', 'mean'),\n",
    "        count_send_txns=('txn_amt', 'count'),\n",
    "        is_esun_from=('is_esun_from', 'max')\n",
    "    ).reset_index().rename(columns={'from_acct': 'acct'})\n",
    "\n",
    "    # --- æ”¶æ¬¾æ–¹ç‰¹å¾µ ---\n",
    "    print(\"   è¨ˆç®—æ”¶æ¬¾æ–¹ç‰¹å¾µ...\")\n",
    "    credit_features = df_txn.groupby('to_acct').agg(\n",
    "        total_recv_amt=('txn_amt', 'sum'),\n",
    "        max_recv_amt=('txn_amt', 'max'),\n",
    "        min_recv_amt=('txn_amt', 'min'),\n",
    "        avg_recv_amt=('txn_amt', 'mean'),\n",
    "        count_recv_txns=('txn_amt', 'count'),\n",
    "        is_esun_to=('is_esun_to', 'max')\n",
    "    ).reset_index().rename(columns={'to_acct': 'acct'})\n",
    "\n",
    "    # --- åˆä½µ ---\n",
    "    print(\"   åˆä½µç‰¹å¾µ...\")\n",
    "    df_features = pd.merge(debit_features, credit_features, on='acct', how='outer')\n",
    "\n",
    "    # å¡«è£œ NaN\n",
    "    fillna_cols = [\n",
    "        'total_send_amt', 'max_send_amt', 'min_send_amt', 'avg_send_amt', 'count_send_txns', 'is_esun_from',\n",
    "        'total_recv_amt', 'max_recv_amt', 'min_recv_amt', 'avg_recv_amt', 'count_recv_txns', 'is_esun_to'\n",
    "    ]\n",
    "    df_features[fillna_cols] = df_features[fillna_cols].fillna(0)\n",
    "    df_features['is_esun'] = df_features[['is_esun_from', 'is_esun_to']].max(axis=1)\n",
    "\n",
    "    print(f\"âœ… ç‰¹å¾µè¨ˆç®—å®Œæˆï¼å…± {df_features.shape[0]} å€‹å¸³æˆ¶ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "new-save-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Combining and saving a single feature file...\n",
      "Combined DataFrame shape: (1800106, 15)\n",
      "âœ… Saved combined data to ./preprocessing_T1_basic.csv\n",
      "ğŸ‰ All steps executed successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- ğŸ’¾ Step 4 & 5 (Revised): Combine and Save a Single File ---\n",
    "print(\"ğŸ’¾ Combining and saving a single feature file...\")\n",
    "\n",
    "# Label the entire feature set first\n",
    "df_features_labeled = pd.merge(df_features, df_alert, on='acct', how='left')\n",
    "# Create the 'label' column: 1 if in alert, 0 otherwise\n",
    "# We can infer this from the non-NaN 'event_date' for alert accounts\n",
    "df_features_labeled['label'] = (~df_features_labeled['event_date'].isnull()).astype(int)\n",
    "\n",
    "# The set of accounts to be predicted\n",
    "predict_acct_set = set(df_predict['acct_test'])\n",
    "\n",
    "# Ensure all accounts from the prediction list are in the main dataframe\n",
    "missing_in_features = predict_acct_set - set(df_features_labeled['acct'])\n",
    "if missing_in_features:\n",
    "    print(f\"âš ï¸ Adding {len(missing_in_features)} accounts from prediction list that had no transactions.\")\n",
    "    missing_df = pd.DataFrame({'acct': list(missing_in_features)})\n",
    "    # Use concat instead of merge for safety with dtypes\n",
    "    df_features_labeled = pd.concat([df_features_labeled, missing_df], ignore_index=True).fillna(0)\n",
    "    # Re-apply label after filling NaNs\n",
    "    df_features_labeled['label'] = df_features_labeled['label'].astype(int)\n",
    "\n",
    "# Now, df_features_labeled contains ALL accounts and their features/labels.\n",
    "# Drop helper columns before saving.\n",
    "df_to_save = df_features_labeled.drop(columns=['event_date'], errors='ignore')\n",
    "\n",
    "print(f\"Combined DataFrame shape: {df_to_save.shape}\")\n",
    "\n",
    "# Save the unified file, which will serve as the input for our PU learning script\n",
    "# We name it 'preprocessing_T1_basic.csv' to match the user's expectation\n",
    "output_path = os.path.join(output_dir, 'preprocessing_T1_basic.csv')\n",
    "df_to_save.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Saved combined data to {output_path}\")\n",
    "print(\"ğŸ‰ All steps executed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
