{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4abe09a9",
   "metadata": {},
   "source": [
    "## ä¸€ã€è¼‰å…¥å¥—ä»¶èˆ‡è¨­å®šè·¯å¾‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12738cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ è³‡æ–™è®€å–è·¯å¾‘è¨­å®šç‚º: ../DataSet/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# å¿½ç•¥ pandas çš„è­¦å‘Šè¨Šæ¯\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- ğŸ“‚ Step 1: è¨­å®šæª”æ¡ˆè·¯å¾‘ ---\n",
    "# æ ¹æ“šæ‚¨çš„æŒ‡ç¤ºï¼Œç›´æ¥è¨­å®šç›¸å°è·¯å¾‘\n",
    "data_dir = \"../DataSet/\"\n",
    "output_dir = \"./\" # è¼¸å‡ºåˆ°ç•¶å‰ç›®éŒ„\n",
    "\n",
    "# ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(f\"ğŸ“‚ è³‡æ–™è®€å–è·¯å¾‘è¨­å®šç‚º: {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8236c2b8",
   "metadata": {},
   "source": [
    "## äºŒã€è®€å–åŸå§‹è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14bd480b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ æ­£åœ¨è®€å–è³‡æ–™...\n",
      "âœ… è³‡æ–™è®€å–å®Œæˆï¼\n",
      "   - äº¤æ˜“ç´€éŒ„: 4435890 ç­†\n",
      "   - è­¦ç¤ºå¸³æˆ¶: 1004 ç­†\n",
      "   - å¾…é æ¸¬å¸³æˆ¶: 4780 ç­†\n"
     ]
    }
   ],
   "source": [
    "# --- ğŸ“¥ Step 2: è®€å–è³‡æ–™ ---\n",
    "print(\"â³ æ­£åœ¨è®€å–è³‡æ–™...\")\n",
    "\n",
    "try:\n",
    "    # 1. è®€å–äº¤æ˜“ç´€éŒ„\n",
    "    txn_path = os.path.join(data_dir, 'acct_transaction.csv')\n",
    "    df_txn = pd.read_csv(txn_path, dtype={\n",
    "        'from_acct': str, 'to_acct': str, \n",
    "        'from_acct_type': str, 'to_acct_type': str,\n",
    "        'txn_amt': float\n",
    "    })\n",
    "\n",
    "    # 2. è®€å–è­¦ç¤ºå¸³æˆ¶ (æ¨™ç±¤è³‡æ–™)\n",
    "    alert_path = os.path.join(data_dir, 'acct_alert.csv')\n",
    "    df_alert = pd.read_csv(alert_path, dtype={'acct': str})\n",
    "\n",
    "    # 3. è®€å–å¾…é æ¸¬å¸³æˆ¶ (æäº¤ç›®æ¨™)\n",
    "    # åŠ å…¥æ¬„ä½åç¨±æª¢æŸ¥æ©Ÿåˆ¶ï¼Œç¢ºä¿èƒ½æ­£ç¢ºè®€å– ID\n",
    "    predict_path = os.path.join(data_dir, 'acct_predict.csv')\n",
    "    # å…ˆè®€å– header ç¢ºèªæ¬„ä½\n",
    "    df_predict_raw = pd.read_csv(predict_path, nrows=0)\n",
    "    predict_cols = df_predict_raw.columns.tolist()\n",
    "    \n",
    "    # åˆ¤æ–· ID æ¬„ä½åç¨±\n",
    "    if 'acct_test' in predict_cols:\n",
    "        pid = 'acct_test'\n",
    "    elif 'acct' in predict_cols:\n",
    "        pid = 'acct'\n",
    "    else:\n",
    "        pid = predict_cols[0] # é è¨­ç¬¬ä¸€å€‹æ¬„ä½\n",
    "    \n",
    "    df_predict = pd.read_csv(predict_path, dtype={pid: str})\n",
    "    if pid != 'acct_test':\n",
    "        df_predict = df_predict.rename(columns={pid: 'acct_test'})\n",
    "\n",
    "    print(f\"âœ… è³‡æ–™è®€å–å®Œæˆï¼\")\n",
    "    print(f\"   - äº¤æ˜“ç´€éŒ„: {df_txn.shape[0]} ç­†\")\n",
    "    print(f\"   - è­¦ç¤ºå¸³æˆ¶: {df_alert.shape[0]} ç­†\")\n",
    "    print(f\"   - å¾…é æ¸¬å¸³æˆ¶: {df_predict.shape[0]} ç­†\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆï¼Œè«‹ç¢ºèªè·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚\\nè©³ç´°éŒ¯èª¤: {e}\")\n",
    "    # ç‚ºäº†è®“ä¸‹é¢çš„ cell ä¸å ±éŒ¯ï¼Œå®šç¾©ç©ºçš„ DataFrame (åƒ…åœ¨å‡ºéŒ¯æ™‚)\n",
    "    df_txn = pd.DataFrame()\n",
    "    df_alert = pd.DataFrame()\n",
    "    df_predict = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100ff2b",
   "metadata": {},
   "source": [
    "## ä¸‰ã€ç‰¹å¾µå·¥ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e09750f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ é–‹å§‹è¨ˆç®— V4 å®˜æ–¹ç‰¹å¾µ...\n",
      "   è¨ˆç®—ä»˜æ¬¾æ–¹ç‰¹å¾µ...\n",
      "   è¨ˆç®—æ”¶æ¬¾æ–¹ç‰¹å¾µ...\n",
      "   åˆä½µç‰¹å¾µ...\n",
      "âœ… ç‰¹å¾µè¨ˆç®—å®Œæˆï¼å…± 1800106 å€‹å¸³æˆ¶ã€‚\n"
     ]
    }
   ],
   "source": [
    "# --- ğŸ› ï¸ Step 3: ç‰¹å¾µå·¥ç¨‹ (Feature Engineering) ---\n",
    "if not df_txn.empty:\n",
    "    print(\"âš™ï¸ é–‹å§‹è¨ˆç®— V4 å®˜æ–¹ç‰¹å¾µ...\")\n",
    "\n",
    "    # 1. è™•ç†ç‰å±±å¸³æˆ¶æ¨™è¨˜\n",
    "    df_txn['is_esun_from'] = (df_txn['from_acct_type'] == '01').astype(int)\n",
    "    df_txn['is_esun_to'] = (df_txn['to_acct_type'] == '01').astype(int)\n",
    "\n",
    "    # --- ä»˜æ¬¾æ–¹ç‰¹å¾µ ---\n",
    "    print(\"   è¨ˆç®—ä»˜æ¬¾æ–¹ç‰¹å¾µ...\")\n",
    "    debit_features = df_txn.groupby('from_acct').agg(\n",
    "        total_send_amt=('txn_amt', 'sum'),\n",
    "        max_send_amt=('txn_amt', 'max'),\n",
    "        min_send_amt=('txn_amt', 'min'),\n",
    "        avg_send_amt=('txn_amt', 'mean'),\n",
    "        count_send_txns=('txn_amt', 'count'),\n",
    "        is_esun_from=('is_esun_from', 'max')\n",
    "    ).reset_index().rename(columns={'from_acct': 'acct'})\n",
    "\n",
    "    # --- æ”¶æ¬¾æ–¹ç‰¹å¾µ ---\n",
    "    print(\"   è¨ˆç®—æ”¶æ¬¾æ–¹ç‰¹å¾µ...\")\n",
    "    credit_features = df_txn.groupby('to_acct').agg(\n",
    "        total_recv_amt=('txn_amt', 'sum'),\n",
    "        max_recv_amt=('txn_amt', 'max'),\n",
    "        min_recv_amt=('txn_amt', 'min'),\n",
    "        avg_recv_amt=('txn_amt', 'mean'),\n",
    "        count_recv_txns=('txn_amt', 'count'),\n",
    "        is_esun_to=('is_esun_to', 'max')\n",
    "    ).reset_index().rename(columns={'to_acct': 'acct'})\n",
    "\n",
    "    # --- åˆä½µ ---\n",
    "    print(\"   åˆä½µç‰¹å¾µ...\")\n",
    "    df_features = pd.merge(debit_features, credit_features, on='acct', how='outer')\n",
    "\n",
    "    # å¡«è£œ NaN\n",
    "    fillna_cols = [\n",
    "        'total_send_amt', 'max_send_amt', 'min_send_amt', 'avg_send_amt', 'count_send_txns', 'is_esun_from',\n",
    "        'total_recv_amt', 'max_recv_amt', 'min_recv_amt', 'avg_recv_amt', 'count_recv_txns', 'is_esun_to'\n",
    "    ]\n",
    "    df_features[fillna_cols] = df_features[fillna_cols].fillna(0)\n",
    "    df_features['is_esun'] = df_features[['is_esun_from', 'is_esun_to']].max(axis=1)\n",
    "\n",
    "    print(f\"âœ… ç‰¹å¾µè¨ˆç®—å®Œæˆï¼å…± {df_features.shape[0]} å€‹å¸³æˆ¶ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf22a65",
   "metadata": {},
   "source": [
    "## è£½ä½œè¨“ç·´é›†èˆ‡æ¸¬è©¦é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "801a4bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ åˆ†å‰²è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†...\n",
      "âœ… åˆ†å‰²å®Œæˆï¼ Train: (1795326, 16), Test: (4780, 17)\n"
     ]
    }
   ],
   "source": [
    "    # --- âœ‚ï¸ Step 4: è³‡æ–™åˆ†å‰² (Data Split) ---\n",
    "    print(\"ğŸ“¦ åˆ†å‰²è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†...\")\n",
    "\n",
    "    # æ¨™è¨˜æ¨™ç±¤\n",
    "    df_features_labeled = pd.merge(df_features, df_alert, on='acct', how='left', indicator=True)\n",
    "    df_features_labeled['label'] = (df_features_labeled['_merge'] == 'both').astype(int)\n",
    "    df_features_labeled = df_features_labeled.drop(columns=['_merge'])\n",
    "\n",
    "    predict_acct_set = set(df_predict['acct_test'])\n",
    "\n",
    "    # æ¸¬è©¦é›† (Test)\n",
    "    df_test = df_features_labeled[df_features_labeled['acct'].isin(predict_acct_set)].copy()\n",
    "    \n",
    "    # è£œé½Šéºæ¼å¸³æˆ¶\n",
    "    missing = predict_acct_set - set(df_test['acct'])\n",
    "    if missing:\n",
    "        print(f\"âš ï¸ è£œé½Š {len(missing)} å€‹ç„¡äº¤æ˜“ç´€éŒ„å¸³æˆ¶ã€‚\")\n",
    "        miss_df = pd.DataFrame({'acct': list(missing)})\n",
    "        for c in df_test.columns:\n",
    "            if c != 'acct': miss_df[c] = 0\n",
    "        df_test = pd.concat([df_test, miss_df], ignore_index=True)\n",
    "    \n",
    "    # ç¢ºä¿é †åºä¸¦ç§»é™¤ä¸å¿…è¦æ¬„ä½\n",
    "    df_test = pd.merge(df_predict, df_test, left_on='acct_test', right_on='acct', how='left')\n",
    "    # ä¿®æ”¹å¾Œ (ç©©å¥å¯«æ³•)\n",
    "    # errors='ignore' ä»£è¡¨å¦‚æœæ¬„ä½ä¸å­˜åœ¨ï¼Œä¸è¦å ±éŒ¯ï¼Œç¹¼çºŒåŸ·è¡Œ\n",
    "    df_test = df_test.drop(columns=['acct_test', 'label'], errors='ignore')\n",
    "\n",
    "    \n",
    "    # é‡æ–°å‘½å acct ç‚º acct_test ä»¥çµ±ä¸€æ¨™ç±¤\n",
    "    df_test = df_test.rename(columns={'acct': 'acct_test'})\n",
    "\n",
    "    # è¨“ç·´é›† (Train)\n",
    "    df_train = df_features_labeled[~df_features_labeled['acct'].isin(predict_acct_set)].copy()\n",
    "\n",
    "    print(f\"âœ… åˆ†å‰²å®Œæˆï¼ Train: {df_train.shape}, Test: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29b75f",
   "metadata": {},
   "source": [
    "## å„²å­˜è™•ç†å¾Œçš„æª”æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2476526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ å„²å­˜æª”æ¡ˆ...\n",
      "ğŸ‰ æ‰€æœ‰æ­¥é©ŸåŸ·è¡Œå®Œç•¢ï¼\n"
     ]
    }
   ],
   "source": [
    "# --- ğŸ’¾ Step 5: å„²å­˜æª”æ¡ˆ ---\n",
    "print(\"ğŸ’¾ å„²å­˜æª”æ¡ˆ...\")\n",
    "df_train.to_csv(os.path.join(output_dir, 'train_features_v4.csv'), index=False)\n",
    "df_test.to_csv(os.path.join(output_dir, 'test_features_v4.csv'), index=False)\n",
    "print(\"ğŸ‰ æ‰€æœ‰æ­¥é©ŸåŸ·è¡Œå®Œç•¢ï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
